# -*- coding: utf-8 -*-
"""PHRED_TRANSFORMER_GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tm-0jN51ntcH8LnljhP7S_hR0g10B2Hj

### Libraries
"""

import tensorflow as tf
from transformers import T5Tokenizer, TFT5ForConditionalGeneration
import numpy as np
from google.colab import drive
import matplotlib.pyplot as plt

"""### GPU"""

DEVICE_NAME = tf.test.gpu_device_name()
if DEVICE_NAME != '/device:GPU:0':
    print('GPU device not found. Training on CPU.')
else:
    print('Found GPU at: {}'.format(DEVICE_NAME))

"""### Drive"""

# Mount Google Drive
drive.mount('/content/drive')

"""### Hyperparameters"""

BATCH_SIZE = 32
SEQ_LENGTH = 100
NUM_EPOCHS = 10
NUM_SAMPLES = 10

"""### Load Dataset."""

data_path = "/content/drive/MyDrive/BIOINFORMATICS/THESIS_KECHAGIAS/DATA/DATASET/phred300_tf_dataset"
dataset = tf.data.Dataset.load(data_path)

iterator = iter(dataset)
first_element = next(iterator)
print(first_element)

def plot_quality_distributions(tf_dataset):
    for batch, qualities_batch in enumerate(tf_dataset):
        for i in range(qualities_batch.shape[0]):
            plt.hist(qualities_batch[i,:,0], bins=20)
            plt.title(f"Quality Score Distribution for Sequence {i} in Batch {batch}")
            plt.xlabel("Quality Score")
            plt.ylabel("Frequency")
            plt.show()

#plot_quality_distributions(dataset)

"""### Tokenize dataset.

"""

tokenizer = T5Tokenizer.from_pretrained("t5-small")

"""### Convert numerical Phred scores back to characters (Sanger encoding)"""

def preprocess_function(sequence):
    # Convert numerical Phred scores back to characters (Sanger encoding)
    seq_str = tf.numpy_function(
        lambda x: "".join(chr(int(s * 93 + 33)) for s in x.numpy().flatten()),
        [sequence],  # Input to the NumPy function
        tf.string)   # Output data type

    return seq_str

text_dataset = dataset.map(
    lambda x: tf.py_function(preprocess_function, [x], Tout=tf.string)
).batch(BATCH_SIZE)

"""### Initialize model"""

model = TFT5ForConditionalGeneration.from_pretrained("t5-small")

"""### Train the model"""

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(optimizer='adam', loss=loss)

model.fit(text_dataset, epochs=NUM_EPOCHS)

"""### Generate synthetic phred scores"""

input_text = "generate phred quality scores:"  # Prompt for the model
input_ids = tokenizer.encode(input_text, return_tensors="tf")
output_sequences = model.generate(
    input_ids=input_ids,
    max_length=SEQ_LENGTH,
    num_return_sequences=NUM_SAMPLES,
    temperature=0.7,  # Control randomness (0.7 is a good starting point)
)

"""### Convert IDs to phred scores"""

for i, output_sequence in enumerate(output_sequences):
    text = tokenizer.decode(output_sequence, skip_special_tokens=True)
    phred_scores = [ord(char) - 33 for char in text]  # Convert back to numeric Phred
    normalized_phred_scores = [score / 93.0 for score in phred_scores]

    # Display or Save
    print(f"Generated Sample {i + 1}:")
    print("Phred Scores:", phred_scores)
    print("Normalized Phred Scores:", normalized_scores)
    print("-" * 20)

"""github.com/tensorflow/model-optimization subject to license (Apache - 2.0)
tensorflow.classcat.com/2021/05/11/huggingface-transformers-4-5-training/
"""